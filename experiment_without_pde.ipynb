{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# CUDA for PyTorch\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
        "%cd stylegan2-ada-pytorch\n",
        "\n",
        "!pip install ninja imageio-ffmpeg\n",
        "\n",
        "!pip install requests\n",
        "\n",
        "!wget -q https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res256-mirror-stylegan2-noaug.pkl \\\n",
        "      -O ffhq256.pkl\n",
        "\n",
        "!wget --content-disposition \\\n",
        "  \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\" \\\n",
        "  -O ffhq256.pkl\n",
        "\n",
        "!ls -lh ffhq256.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xPy495wj2vr"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import torch\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "with open('ffhq256.pkl', 'rb') as f:\n",
        "    G = pickle.load(f)['G_ema'].to(device)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "z = torch.randn(1, G.z_dim).to(device)\n",
        "w = G.mapping(z, None)  # latent space (can be w or w+)\n",
        "\n",
        "# noise_mode \\in ['const', 'random', 'none']\n",
        "\n",
        "img = G.synthesis(w, noise_mode='const')  # [1, 3, 256, 256]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CelebA-HQ 256x256 Download\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# 1. kaggle.json file\n",
        "kaggle_json_path = \"/home/user_yjs/DGM_Project/kaggle.json\" \n",
        "kaggle_dir = os.path.expanduser(\"~/.kaggle\")\n",
        "os.makedirs(kaggle_dir, exist_ok=True)\n",
        "shutil.copy(kaggle_json_path, os.path.join(kaggle_dir, \"kaggle.json\"))\n",
        "os.chmod(os.path.join(kaggle_dir, \"kaggle.json\"), 0o600)\n",
        "\n",
        "# 2. import kaggle library\n",
        "try:\n",
        "    import kaggle\n",
        "except ImportError:\n",
        "    import subprocess\n",
        "    subprocess.check_call([\"pip\", \"install\", \"kaggle\"])\n",
        "    import kaggle\n",
        "\n",
        "# 3. Download & Unzip\n",
        "dataset_name = \"badasstechie/celebahq-resized-256x256\"\n",
        "zip_name = \"celebahq-resized-256x256.zip\"\n",
        "output_dir = \"celebahq_256\"\n",
        "\n",
        "# Download\n",
        "if not os.path.exists(zip_name):\n",
        "    os.system(f\"kaggle datasets download -d {dataset_name}\")\n",
        "\n",
        "# Unzip\n",
        "if not os.path.exists(output_dir):\n",
        "    with zipfile.ZipFile(zip_name, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(output_dir)\n",
        "\n",
        "print(f\"Download and unzip completed: {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# imgae path\n",
        "img_path = \"/home/user_yjs/DGM_Project/stylegan2-ada-pytorch/celebahq_256/celeba_hq_256/00013.jpg\"\n",
        "\n",
        "# image load and post proccessing\n",
        "transform = T.Compose([\n",
        "    T.Resize((256, 256)),\n",
        "    T.ToTensor(),  # [0, 1]\n",
        "])\n",
        "img = Image.open(img_path).convert(\"RGB\")\n",
        "img_tensor = transform(img)  # (3, 256, 256)\n",
        "\n",
        "# centered mask\n",
        "mask = torch.zeros(1, 256, 256)\n",
        "mask[:, 80:176, 80:176] = 1.0\n",
        "masked_img_tensor = img_tensor * (1 - mask)\n",
        "\n",
        "# Encoder input: masked image + mask\n",
        "encoder_input = torch.cat([masked_img_tensor, mask], dim=0).unsqueeze(0)  # shape: (1, 4, 256, 256)\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(img_tensor.permute(1, 2, 0).numpy())\n",
        "plt.title(\"Original\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(mask[0], cmap='gray')\n",
        "plt.title(\"Mask\")\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(masked_img_tensor.permute(1, 2, 0).numpy())\n",
        "plt.title(\"Masked Image\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H01HiZGbkIcp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Conv and downsample block\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_c, out_c, 3, 1, 1),\n",
        "            nn.GroupNorm(32, out_c),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(out_c, out_c, 3, 1, 1),\n",
        "            nn.GroupNorm(32, out_c),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "    def forward(self, x):  return self.block(x)\n",
        "\n",
        "class DownsampleBlock(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_c, out_c, 4, 2, 1),\n",
        "            nn.GroupNorm(32, out_c),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "    def forward(self, x):  return self.block(x)\n",
        "\n",
        "# Encoder\n",
        "class InvertFillEncoder(nn.Module):\n",
        "    def __init__(self, in_channels: int = 4):\n",
        "        super().__init__()\n",
        "\n",
        "        # extracting feature\n",
        "        self.conv1 = ConvBlock(in_channels, 64)        # res0 : 256×256\n",
        "        self.down1 = DownsampleBlock(64, 128)          # res1 : 128×128\n",
        "        self.down2 = DownsampleBlock(128, 256)         # res2 :  64×64\n",
        "        self.down3 = DownsampleBlock(256, 256)         # res3 :  32×32\n",
        "        self.down4 = DownsampleBlock(256, 512)         # res4 :  16×16\n",
        "        self.down5 = DownsampleBlock(512, 512)         # res5 :   8×8\n",
        "        self.down6 = DownsampleBlock(512, 512)         # res6 :   4×4   \n",
        "\n",
        "        # multi-scale RGB heads  \n",
        "        self.rgb_heads = nn.ModuleList([\n",
        "            nn.Conv2d( 64, 3, 1),   # res0 256×256\n",
        "            nn.Conv2d(128, 3, 1),   # res1 128×128\n",
        "            nn.Conv2d(256, 3, 1),   # res2  64×64\n",
        "            nn.Conv2d(256, 3, 1),   # res3  32×32\n",
        "            nn.Conv2d(512, 3, 1),   # res4  16×16\n",
        "            nn.Conv2d(512, 3, 1),   # res5   8×8\n",
        "            nn.Conv2d(512, 3, 1),   # res6   4×4\n",
        "        ])\n",
        "\n",
        "        # RGB heads\n",
        "        self.rgb1 = nn.Conv2d(128, 3, 1)   # from res1 (128×128)\n",
        "        self.rgb2 = nn.Conv2d(256, 3, 1)   # from res3 (32×32)\n",
        "        self.rgb3 = nn.Conv2d(512, 3, 1)   # from res5 (8×8)\n",
        "\n",
        "    # ─────────────────────────────────────────\n",
        "    def forward(self, x):\n",
        "        feats = {}\n",
        "\n",
        "        x = self.conv1(x);  feats[\"res0\"] = x          # 256\n",
        "        x = self.down1(x);  feats[\"res1\"] = x          # 128\n",
        "        x = self.down2(x);  feats[\"res2\"] = x          #  64\n",
        "        x = self.down3(x);  feats[\"res3\"] = x          #  32\n",
        "        x = self.down4(x);  feats[\"res4\"] = x          #  16\n",
        "        x = self.down5(x);  feats[\"res5\"] = x          #   8\n",
        "        x = self.down6(x);  feats[\"res6\"] = x          #   4\n",
        "\n",
        "        # multi-scale RGB projection\n",
        "        Or_E = [head(feats[f\"res{i}\"]) for i, head in enumerate(self.rgb_heads)]\n",
        "\n",
        "        return feats[\"res6\"], [feats[f\"res{i}\"] for i in range(7)], Or_E"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7mOSoaZkLii"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Feature >> style vector\n",
        "class Map2Style(nn.Module):\n",
        "    def __init__(self, in_channels: int, style_dim: int = 512):\n",
        "        super().__init__()\n",
        "        self.mapping = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 512, 3, 1, 1),\n",
        "            nn.ReLU(True),\n",
        "            nn.AdaptiveAvgPool2d(1),   # → [B, 512, 1, 1]\n",
        "            nn.Flatten(),              # → [B, 512]\n",
        "            nn.Linear(512, style_dim)  # → [B, style_dim]\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.mapping(x)         # [B, 512]\n",
        "\n",
        "# Style Head\n",
        "class Map2StyleHead(nn.Module):\n",
        "    def __init__(self, style_dim: int = 512):\n",
        "        super().__init__()\n",
        "        self.map_fine0    = Map2Style( 64,  style_dim)   # res0 256×256\n",
        "        self.map_fine1    = Map2Style(128,  style_dim)   # res1 128×128\n",
        "        self.map_middle0  = Map2Style(256,  style_dim)   # res2  64×64\n",
        "        self.map_middle1  = Map2Style(256,  style_dim)   # res3  32×32\n",
        "        self.map_coarse0  = Map2Style(512,  style_dim)   # res4  16×16\n",
        "        self.map_coarse1  = Map2Style(512,  style_dim)   # res5   8×8\n",
        "        self.map_coarse2  = Map2Style(512,  style_dim)   # res6   4×4\n",
        "\n",
        "    def forward(self, feats):          \n",
        "        w_fine0    = self.map_fine0   (feats[0])   # 256\n",
        "        w_fine1    = self.map_fine1   (feats[1])   # 128\n",
        "        w_middle0  = self.map_middle0 (feats[2])   #  64\n",
        "        w_middle1  = self.map_middle1 (feats[3])   #  32\n",
        "        w_coarse0  = self.map_coarse0 (feats[4])   #  16\n",
        "        w_coarse1  = self.map_coarse1 (feats[5])   #   8\n",
        "        w_coarse2  = self.map_coarse2 (feats[6])   #   4\n",
        "\n",
        "        return [w_coarse2, w_coarse1, w_coarse0,\n",
        "                w_middle1, w_middle0,\n",
        "                w_fine1,   w_fine0]                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-NPqeXykPoY"
      },
      "outputs": [],
      "source": [
        "# RGB proj >> Structure\n",
        "class Map2Structure(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_dim: int = 512):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 512, 3, 1, 1),\n",
        "            nn.ReLU(True),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512, out_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)         # [B, 512]\n",
        "\n",
        "# Structure Head\n",
        "class Map2StructureHead(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.map_fine0    = Map2Structure(3)  # res0 256\n",
        "        self.map_fine1    = Map2Structure(3)  # res1 128\n",
        "        self.map_middle0  = Map2Structure(3)  # res2  64\n",
        "        self.map_middle1  = Map2Structure(3)  # res3  32\n",
        "        self.map_coarse0  = Map2Structure(3)  # res4  16\n",
        "        self.map_coarse1  = Map2Structure(3)  # res5   8\n",
        "        self.map_coarse2  = Map2Structure(3)  # res6   4\n",
        "\n",
        "    def forward(self, images):         \n",
        "        s_fine0    = self.map_fine0   (images[0])\n",
        "        s_fine1    = self.map_fine1   (images[1])\n",
        "        s_middle0  = self.map_middle0 (images[2])\n",
        "        s_middle1  = self.map_middle1 (images[3])\n",
        "        s_coarse0  = self.map_coarse0 (images[4])\n",
        "        s_coarse1  = self.map_coarse1 (images[5])\n",
        "        s_coarse2  = self.map_coarse2 (images[6])\n",
        "\n",
        "        # coarse(4>>16) >> middle(32>>64) >> fine(128>>256)\n",
        "        return [s_coarse2, s_coarse1, s_coarse0,\n",
        "                s_middle1, s_middle0,\n",
        "                s_fine1,   s_fine0]                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mi5HfgKnkRWd"
      },
      "outputs": [],
      "source": [
        "encoder = InvertFillEncoder()\n",
        "map2structure = Map2StructureHead()\n",
        "map2style = Map2StyleHead()\n",
        "x = torch.randn(1, 4, 256, 256)  # [B=1, C=4, H=1024, W=1024]\n",
        "\n",
        "# x: 4channel masked input image\n",
        "f_r, feats_for_structure, Or_E = encoder(x)\n",
        "S_r = map2structure(Or_E)  # S_r = [S_coarse, S_middle, S_fine]\n",
        "\n",
        "# Style latent vector w' = [w'_0, w'_1, w'_2]\n",
        "w_primes = map2style(feats_for_structure)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Modulation\n",
        "class FullPreModulationNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    StyleGAN2-ADA 256×256:\n",
        "        - Resolution stages: 4,8,16,32,64,128,256  → 7\n",
        "        - num of layers      : 7 × 2 conv           → 14\n",
        "    input\n",
        "        w_primes : (B, 7, 512)  coarse2 … fine0\n",
        "        S_r      : (B, 7, 512)\n",
        "    output\n",
        "        w*       : (B, 14, 512)\n",
        "    \"\"\"\n",
        "    def __init__(self, style_dim: int = 512):\n",
        "        super().__init__()\n",
        "        self.num_layers = 14      \n",
        "        self.style_dim  = style_dim\n",
        "\n",
        "        # step-1  : w'  >> Dense × 2 >> IN\n",
        "        self.w_proj  = nn.Sequential(\n",
        "            nn.Linear(style_dim, style_dim), nn.ReLU(True),\n",
        "            nn.Linear(style_dim, style_dim), nn.ReLU(True),\n",
        "        )\n",
        "        self.inst_norm = nn.LayerNorm(style_dim)\n",
        "\n",
        "        # step-2  : [S_r ‖ w_r] >> Dense >> gamma, beta\n",
        "        self.joint_proj = nn.Linear(style_dim * 2, style_dim)\n",
        "        self.to_gamma   = nn.Linear(style_dim, style_dim)\n",
        "        self.to_beta    = nn.Linear(style_dim, style_dim)\n",
        "\n",
        "        # level_map:  [0,0,1,1,2,2,3,3,4,4,5,5,6,6]\n",
        "        self.level_map = [i for i in range(7) for _ in range(2)]\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    def forward(self, w_primes: torch.Tensor, S_r: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        w_primes : (B, 7, 512)\n",
        "        S_r      : (B, 7, 512)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        w_star   : (B, 14, 512)\n",
        "        \"\"\"\n",
        "        B = w_primes.size(0)\n",
        "        w_star_layers = []\n",
        "\n",
        "        for l in range(self.num_layers):\n",
        "            r = self.level_map[l]       # 0~6\n",
        "            w_r = w_primes[:, r, :]     # (B, 512)\n",
        "            s_r = S_r[:, r, :]          # (B, 512)\n",
        "\n",
        "            # Step-1  : w' >> projection + instance-norm\n",
        "            w_proj = self.w_proj(w_r)          # (B, 512)\n",
        "            w_norm = self.inst_norm(w_proj)\n",
        "\n",
        "            # Step-2  : [s_r ‖ w_r] >> gamma, beta\n",
        "            joint  = torch.cat([s_r, w_r], dim=1)   # (B, 1024)\n",
        "            h      = F.relu(self.joint_proj(joint)) # (B, 512)\n",
        "            gamma  = self.to_gamma(h)\n",
        "            beta   = self.to_beta(h)\n",
        "\n",
        "            # Step-3  : gamma dot IN(w') + beta\n",
        "            w_star = gamma * w_norm + beta\n",
        "            w_star_layers.append(w_star)\n",
        "\n",
        "        return torch.stack(w_star_layers, dim=1)     # (B, 14, 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# randomly input\n",
        "w_primes = torch.randn(1, 7, 512)   # coarse2 … fine0\n",
        "S_r      = torch.randn(1, 7, 512)\n",
        "\n",
        "full_premod = FullPreModulationNetwork()\n",
        "full_premod.eval()\n",
        "\n",
        "w_star_all = full_premod(w_primes, S_r)      # (1, 14, 512)\n",
        "print(\"w_star_all shape:\", w_star_all.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pre-modulation network implementation\n",
        "w_star_all = full_premod(w_primes, S_r)  # (1, 18, 512)\n",
        "\n",
        "# output check\n",
        "print(\"w_star_all shape:\", w_star_all.shape)\n",
        "\n",
        "for l in range(w_star_all.shape[1]):\n",
        "    print(f\"w*_{l}:\", w_star_all[0, l, :5])  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJM2V3f_kexA"
      },
      "outputs": [],
      "source": [
        "# Selected sample\n",
        "w_star_sample = w_star_all.to('cuda')  # Convert to GPU\n",
        "\n",
        "# Generate image\n",
        "img = G.synthesis(w_star_sample, noise_mode='const')  # (3, H, W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "m5dgA4OzkjDp"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eN7bK2rlkkrY"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torchvision.models import vgg16, VGG16_Weights\n",
        "\n",
        "# VGG16 ─ perceptual·style backbone\n",
        "class SharedVGG(nn.Module):\n",
        "    \"\"\"\n",
        "    Used layers\n",
        "      relu1_2, relu2_2, relu3_3  \n",
        "    output: List[Tensor]  length 3\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg = vgg16(weights=VGG16_Weights.DEFAULT).features\n",
        "        self.layers = nn.ModuleList([\n",
        "            vgg[:4],     # relu1_2\n",
        "            vgg[4:9],    # relu2_2\n",
        "            vgg[9:16],   # relu3_3\n",
        "        ])\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = []\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "            feats.append(x)\n",
        "        return feats                         # 3-scale feature\n",
        "\n",
        "# 2. Perceptual  &  Style  Loss\n",
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self, shared_vgg):\n",
        "        super().__init__()\n",
        "        self.vgg = shared_vgg\n",
        "\n",
        "    def forward(self, inp, tgt):\n",
        "        loss = 0.\n",
        "        for f_i, f_t in zip(self.vgg(inp), self.vgg(tgt)):\n",
        "            loss += F.mse_loss(f_i, f_t)\n",
        "        return loss\n",
        "\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "    def __init__(self, shared_vgg):\n",
        "        super().__init__()\n",
        "        self.vgg = shared_vgg\n",
        "\n",
        "    @staticmethod\n",
        "    def gram(x):\n",
        "        B, C, H, W = x.shape\n",
        "        feat = x.view(B, C, -1)\n",
        "        return torch.bmm(feat, feat.transpose(1, 2)) / (C * H * W)\n",
        "\n",
        "    def forward(self, inp, tgt):\n",
        "        loss = 0.\n",
        "        for f_i, f_t in zip(self.vgg(inp), self.vgg(tgt)):\n",
        "            loss += F.mse_loss(self.gram(f_i), self.gram(f_t))\n",
        "        return loss\n",
        "\n",
        "# Loss of pixel, TV\n",
        "def reconstruction_loss_valid(I_pred, I_gt, mask):\n",
        "    \"\"\"MSE of valid\"\"\"\n",
        "    return F.mse_loss(I_pred * (1 - mask), I_gt * (1 - mask))\n",
        "\n",
        "def reconstruction_loss_hole(I_pred, I_gt, mask):\n",
        "    \"\"\"MSE of hole\"\"\"\n",
        "    return F.mse_loss(I_pred * mask, I_gt * mask)\n",
        "\n",
        "def tv_loss(I_pred):\n",
        "    \"\"\"Total Variation\"\"\"\n",
        "    return (torch.mean(torch.abs(I_pred[:, :, :, :-1] - I_pred[:, :, :,  1:])) +\n",
        "            torch.mean(torch.abs(I_pred[:, :, :-1, :] - I_pred[:, :,  1:, :])))\n",
        "\n",
        "# 4. MSR (multi-scale recon) loss\n",
        "def mse_loss(Im, I_gt_encoder, encoder, map2structure):\n",
        "    \"\"\"\n",
        "    encoder        : InvertFillEncoder (7-scale)\n",
        "    map2structure  : Map2StructureHead (7-scale)\n",
        "    \"\"\"\n",
        "    _, _, Or_E_m  = encoder(Im)            # List len == 7\n",
        "    S_r_m          = map2structure(Or_E_m)\n",
        "\n",
        "    _, _, Or_E_gt = encoder(I_gt_encoder)\n",
        "    S_r_gt         = map2structure(Or_E_gt)\n",
        "\n",
        "    loss_Or_E = sum(F.mse_loss(a, b) for a, b in zip(Or_E_m,  Or_E_gt))\n",
        "    loss_S_r  = sum(F.mse_loss(a, b) for a, b in zip(S_r_m, S_r_gt))\n",
        "    return loss_Or_E + loss_S_r\n",
        "\n",
        "# 5. loss dict \n",
        "shared_vgg = SharedVGG().to(device)\n",
        "\n",
        "losses = {\n",
        "    \"valid\"      : reconstruction_loss_valid,\n",
        "    \"hole\"       : reconstruction_loss_hole,\n",
        "    \"perceptual\" : PerceptualLoss(shared_vgg),\n",
        "    \"style\"      : StyleLoss(shared_vgg),\n",
        "    \"tv\"         : tv_loss,\n",
        "    \"msr\"        : mse_loss,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbTFQ305kn5E"
      },
      "outputs": [],
      "source": [
        "encoder = InvertFillEncoder()\n",
        "map2style = Map2StyleHead()\n",
        "map2structure = Map2StructureHead()\n",
        "premod = FullPreModulationNetwork()\n",
        "\n",
        "\n",
        "with open('ffhq256.pkl', 'rb') as f:\n",
        "    generator = pickle.load(f)['G_ema'].to(device)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJ4SLAOBkqhY"
      },
      "outputs": [],
      "source": [
        "class InpaintingTrainer:\n",
        "    \"\"\"\n",
        "    End-to-End inpainting trainig loop\n",
        "      Encoder          : 7 scales (res0 ~ res6)\n",
        "      Map2Style/Struct : 7 vectors (512 dim)\n",
        "      PreMod           : 14 layers (coarse2 … fine0, conv×2)\n",
        "      Generator        : StyleGAN2-ADA 256,  num_ws = 14\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 encoder, map2style, map2structure,\n",
        "                 premod, generator, losses,\n",
        "                 device: str = \"cuda\"):\n",
        "\n",
        "        self.device        = device\n",
        "        self.encoder       = encoder.to(device)\n",
        "        self.map2style     = map2style.to(device)\n",
        "        self.map2structure = map2structure.to(device)\n",
        "        self.premod        = premod.to(device)\n",
        "        self.generator     = generator.to(device)\n",
        "        self.losses        = losses\n",
        "  \n",
        "        self.num_ws = getattr(generator, \"num_ws\",\n",
        "                              getattr(generator.synthesis, \"num_ws\", 14))\n",
        "\n",
        "        self.optim = torch.optim.Adam(\n",
        "            list(self.encoder.parameters())       +\n",
        "            list(self.map2style.parameters())     +\n",
        "            list(self.map2structure.parameters()) +\n",
        "            list(self.premod.parameters()),\n",
        "            lr = 1e-4\n",
        "        )\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    def train_step(self, Im, I_gt, I_gt_encoder, mask):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        Im           : (B, 4, 256, 256)  ─ [masked RGB + mask]\n",
        "        I_gt         : (B, 3, 256, 256)  ─ ground truth\n",
        "        I_gt_encoder : (B, 4, 256, 256)  ─ GT (mask=0) for MSR loss\n",
        "        mask         : (B, 1, 256, 256)\n",
        "        \"\"\"\n",
        "        Im           = Im.to(self.device)\n",
        "        I_gt         = I_gt.to(self.device)\n",
        "        I_gt_encoder = I_gt_encoder.to(self.device)\n",
        "        mask         = mask.to(self.device)\n",
        "\n",
        "        # Encoder >> 7 scales feature & O_r^E\n",
        "        f_r, feats_for_structure, Or_E = self.encoder(Im)               # len 7\n",
        "\n",
        "        # Extracting Structure / Style\n",
        "        S_r_list       = self.map2structure(Or_E)                       # 7×[B,512]\n",
        "        w_primes_list  = self.map2style(feats_for_structure)            # 7×[B,512]\n",
        "\n",
        "        S_r      = torch.stack(S_r_list,      dim=1)   # (B, 7, 512)\n",
        "        w_primes = torch.stack(w_primes_list, dim=1)   # (B, 7, 512)\n",
        "\n",
        "        # Pre-Mod >> w*  (14 layers)\n",
        "        w_star = self.premod(w_primes, S_r)            # (B, 14, 512)\n",
        "\n",
        "        # padding / truncation \n",
        "        if w_star.shape[1] < self.num_ws:                       # pad\n",
        "            pad = torch.zeros(w_star.size(0),\n",
        "                               self.num_ws - w_star.size(1),\n",
        "                               w_star.size(2),\n",
        "                               device = w_star.device)\n",
        "            w_star = torch.cat([w_star, pad], dim=1)\n",
        "        elif w_star.shape[1] > self.num_ws:                     # truncate\n",
        "            w_star = w_star[:, :self.num_ws]\n",
        "\n",
        "        # Synthesis\n",
        "        I_pred = self.generator.synthesis(w_star, noise_mode=\"const\")   # (B,3,256,256)\n",
        "        I_pred = (I_pred + 1) / 2.0                                     # [-1,1] → [0,1]\n",
        "\n",
        "        # Losses\n",
        "        L_valid  = 10.0  * self.losses[\"valid\"](I_pred, Im[:, :3], mask)\n",
        "        L_hole   =         self.losses[\"hole\"] (I_pred, Im[:, :3], mask)\n",
        "        L_perc   =         self.losses[\"perceptual\"](I_pred, I_gt)\n",
        "        L_style  = 1e6   * self.losses[\"style\"](I_pred, I_gt)\n",
        "        L_tv     =         self.losses[\"tv\"](I_pred)\n",
        "        L_msr    = 1e3   * self.losses[\"msr\"](Im, I_gt_encoder,\n",
        "                                              self.encoder, self.map2structure)\n",
        "\n",
        "        L_total = L_valid + L_hole + L_perc + L_style + L_tv + L_msr\n",
        "\n",
        "        # Back-propagation\n",
        "        self.optim.zero_grad(set_to_none=True)\n",
        "        L_total.backward()\n",
        "        self.optim.step()\n",
        "\n",
        "        return {\n",
        "            \"loss_total\" : L_total.item(),\n",
        "            \"loss_valid\" : L_valid.item(),\n",
        "            \"loss_hole\"  : L_hole.item(),\n",
        "            \"loss_perc\"  : L_perc.item(),\n",
        "            \"loss_style\" : L_style.item(),\n",
        "            \"loss_tv\"    : L_tv.item(),\n",
        "            \"loss_msr\"   : L_msr.item(),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "j_IH1itCksF3"
      },
      "outputs": [],
      "source": [
        "trainer = InpaintingTrainer(\n",
        "    encoder=encoder,\n",
        "    map2style=map2style,\n",
        "    map2structure=map2structure,\n",
        "    premod=premod,\n",
        "    generator=generator,\n",
        "    losses=losses,\n",
        "    device='cuda'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK3cQlf7kufj"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "\n",
        "def train_epochs(trainer, data_list, num_epochs=5000):\n",
        "    logs = {\n",
        "        'loss_total': [], 'loss_valid': [], 'loss_hole': [],\n",
        "        'loss_perc': [], 'loss_style': [], 'loss_tv': [], 'loss_msr': []\n",
        "    }\n",
        "\n",
        "    # tqdm range instead of print per epoch\n",
        "    pbar = trange(num_epochs, desc=\"Training\", unit=\"epoch\")\n",
        "\n",
        "    for epoch in pbar:\n",
        "        epoch_log = {k: 0.0 for k in logs}\n",
        "        for (Im, I_gt, I_gt_encoder, mask) in data_list:\n",
        "            log = trainer.train_step(Im, I_gt, I_gt_encoder, mask)\n",
        "            for k in log:\n",
        "                if log[k] is not None:\n",
        "                    epoch_log[k] += log[k]\n",
        "\n",
        "        for k in logs:\n",
        "            logs[k].append(epoch_log[k] / len(data_list))\n",
        "\n",
        "        # log update\n",
        "        pbar.set_postfix({\n",
        "            'Total': f\"{logs['loss_total'][-1]:.4f}\",\n",
        "            'Recon': f\"{(logs['loss_valid'][-1]+logs['loss_hole'][-1]):.4f}\",\n",
        "            'Perc': f\"{logs['loss_perc'][-1]:.4f}\",\n",
        "            'Style': f\"{logs['loss_style'][-1]:.2e}\",\n",
        "            'TV': f\"{logs['loss_tv'][-1]:.4f}\",\n",
        "            'MSR': f\"{logs['loss_msr'][-1]:.4f}\"\n",
        "        })\n",
        "\n",
        "    return logs\n",
        "\n",
        "def plot_loss_curves(logs):\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for key, values in logs.items():\n",
        "        plt.plot(values, label=key)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Loss Curves\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def visualize_prediction(I_pred, I_gt, I_masked):\n",
        "    def to_np(img):\n",
        "      return img.squeeze(0).permute(1, 2, 0).clamp(0, 1).cpu().detach().numpy()\n",
        "\n",
        "    def to_np_m(img):\n",
        "      img = img.detach().cpu()\n",
        "      img_norm = (img + 1) / 2\n",
        "      img_norm = torch.clamp(img_norm, 0, 1)\n",
        "      img_norm_squ = img_norm.squeeze(0)\n",
        "      img = img_norm_squ.permute(1, 2, 0).numpy()\n",
        "\n",
        "      return img\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(to_np(I_masked))\n",
        "    plt.title(\"Masked Input\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(to_np_m(I_pred))\n",
        "    plt.title(\"Predicted Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(to_np(I_gt))\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHh1HC7Rk00r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "# InpaintingDataset\n",
        "class InpaintingDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_generator, transform, mask_size=64):\n",
        "        self.paths = sorted([\n",
        "            os.path.join(image_dir, f)\n",
        "            for f in os.listdir(image_dir)\n",
        "            if f.lower().endswith(('.jpg', '.png'))\n",
        "        ])\n",
        "        self.t      = transform\n",
        "        self.m_gen  = mask_generator\n",
        "        self.m_size = mask_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        img = self.t(img)                             # (3, 256, 256)\n",
        "\n",
        "        mask = self.m_gen(img.shape[1:], self.m_size) # (1, 256, 256)\n",
        "        Im   = torch.cat([img * (1 - mask), mask], 0) # (4, 256, 256)\n",
        "        I_gt_enc = torch.cat([img, torch.zeros_like(mask)], 0)\n",
        "\n",
        "        return Im, img, I_gt_enc, mask\n",
        "        \n",
        "def center_square_mask(shape, size=64):\n",
        "    H, W = shape\n",
        "    m = torch.zeros(1, H, W)\n",
        "    s = (H-size)//2\n",
        "    m[:, s:s+size, s:s+size] = 1.\n",
        "    return m        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "oGYqKoUjk5pG"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split, DataLoader\n",
        "transform = T.Compose([T.Resize((256,256)), T.ToTensor()])\n",
        "\n",
        "full_ds = InpaintingDataset(\"celebahq_256/celeba_hq_256\",\n",
        "                            center_square_mask, transform, 64)\n",
        "\n",
        "n_train = int(0.02*len(full_ds))\n",
        "train_ds, val_ds = random_split(full_ds, [n_train, len(full_ds)-n_train])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_ds, batch_size=1, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logs = train_epochs(trainer, train_loader, num_epochs=3000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_loss_curves(logs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_prediction(I_pred, I_gt, I_masked):\n",
        "    def to_np(img):\n",
        "      return img.squeeze(0).permute(1, 2, 0).clamp(0, 1).cpu().detach().numpy()\n",
        "\n",
        "    def to_np_m(img):\n",
        "      img = img.detach().cpu()\n",
        "      img_norm = (img + 1) / 2\n",
        "      img_norm = torch.clamp(img_norm, 0, 1)\n",
        "      img_norm_squ = img_norm.squeeze(0)\n",
        "      img = img_norm_squ.permute(1, 2, 0).numpy()\n",
        "\n",
        "      return img\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(to_np(I_masked))\n",
        "    plt.title(\"Masked Input\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(to_np_m(I_pred))\n",
        "    plt.title(\"Predicted Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(to_np(I_gt))\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "# Extract 10 training dataset\n",
        "train10 = DataLoader(Subset(train_ds, range(10)), batch_size=1)\n",
        "\n",
        "for i, (Im, I_gt, I_gt_enc, mask) in enumerate(train10):\n",
        "    Im = Im.to(device)\n",
        "    I_gt = I_gt.to(device)\n",
        "    I_gt_enc = I_gt_enc.to(device)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        f_r, feats, Or_E = trainer.encoder(Im)\n",
        "        S_r = torch.stack(trainer.map2structure(Or_E), dim=1)\n",
        "        w_p = torch.stack(trainer.map2style(feats), dim=1)\n",
        "        w_star = trainer.premod(w_p, S_r)\n",
        "        I_pred = trainer.generator.synthesis(w_star, noise_mode='const')\n",
        "\n",
        "    # Visualization\n",
        "    visualize_prediction(I_pred, I_gt, Im[:, :3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Subset\n",
        "\n",
        "# Extract 10 test dataset\n",
        "val10 = DataLoader(Subset(val_ds, range(10)), batch_size=1)\n",
        "\n",
        "for i, (Im, I_gt, I_gt_enc, mask) in enumerate(val10):\n",
        "    Im = Im.to(device)\n",
        "    I_gt = I_gt.to(device)\n",
        "    I_gt_enc = I_gt_enc.to(device)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        f_r, feats, Or_E = trainer.encoder(Im)\n",
        "        S_r = torch.stack(trainer.map2structure(Or_E), dim=1)\n",
        "        w_p = torch.stack(trainer.map2style(feats), dim=1)\n",
        "        w_star = trainer.premod(w_p, S_r)\n",
        "        I_pred = trainer.generator.synthesis(w_star, noise_mode='const')\n",
        "        #I_pred = (I_pred + 1) / 2  # Normalize to [0,1]\n",
        "\n",
        "    visualize_prediction(I_pred, I_gt, Im[:, :3])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
